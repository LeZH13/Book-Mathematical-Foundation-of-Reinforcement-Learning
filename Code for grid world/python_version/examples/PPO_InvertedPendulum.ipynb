{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1007ae0",
   "metadata": {},
   "source": [
    "# PPO for Mujoco InvertedPendulum-v5\n",
    "\n",
    "This notebook demonstrates Proximal Policy Optimization (PPO) for the Mujoco InvertedPendulum-v5 environment using vectorized environments and PyTorch. Each section is modular and can be run independently, with variables persisting across cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "909a6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Speed up: use vectorized envs\n",
    "def make_env():\n",
    "    return gym.make(\"InvertedPendulum-v5\", render_mode=None, disable_env_checker=True)\n",
    "\n",
    "envs = gym.vector.AsyncVectorEnv([make_env for _ in range(8)])  # 8 parallel envs\n",
    "obs_dim = envs.single_observation_space.shape[0]\n",
    "act_dim = envs.single_action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c4cb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64), nn.Tanh(),\n",
    "            nn.Linear(64, 64), nn.Tanh(),\n",
    "            nn.Linear(64, act_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64), nn.Tanh(),\n",
    "            nn.Linear(64, 64), nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3fdb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_and_value(model, obs, device):\n",
    "    obs_tensor = torch.from_numpy(obs).float().to(device)\n",
    "    mu, value = model(obs_tensor)\n",
    "    std = model.log_std.exp()\n",
    "    dist = torch.distributions.Normal(mu, std)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action).sum(-1)\n",
    "    return action.cpu().detach().numpy(), log_prob.cpu().detach().numpy(), value.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b77bff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print which device is being used\n",
    "print(f\"Using device: {device}\")\n",
    "model = ActorCritic(obs_dim, act_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# PPO hyperparameters\n",
    "clip_eps = 0.2\n",
    "epochs = 10\n",
    "steps_per_epoch = 2048\n",
    "gamma = 0.99\n",
    "lam = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e36038ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 0: mean reward 0.78369140625\n",
      "Update 1: mean reward 0.7960205078125\n",
      "Update 1: mean reward 0.7960205078125\n",
      "Update 2: mean reward 0.81549072265625\n",
      "Update 2: mean reward 0.81549072265625\n",
      "Update 3: mean reward 0.835693359375\n",
      "Update 3: mean reward 0.835693359375\n",
      "Update 4: mean reward 0.8519287109375\n",
      "Update 4: mean reward 0.8519287109375\n",
      "Update 5: mean reward 0.871826171875\n",
      "Update 5: mean reward 0.871826171875\n",
      "Update 6: mean reward 0.88946533203125\n",
      "Update 6: mean reward 0.88946533203125\n",
      "Update 7: mean reward 0.9039306640625\n",
      "Update 7: mean reward 0.9039306640625\n",
      "Update 8: mean reward 0.916015625\n",
      "Update 8: mean reward 0.916015625\n",
      "Update 9: mean reward 0.92535400390625\n",
      "Update 9: mean reward 0.92535400390625\n",
      "Update 10: mean reward 0.93359375\n",
      "Update 10: mean reward 0.93359375\n",
      "Update 11: mean reward 0.9417724609375\n",
      "Update 11: mean reward 0.9417724609375\n",
      "Update 12: mean reward 0.9488525390625\n",
      "Update 12: mean reward 0.9488525390625\n",
      "Update 13: mean reward 0.954345703125\n",
      "Update 13: mean reward 0.954345703125\n",
      "Update 14: mean reward 0.9578857421875\n",
      "Update 14: mean reward 0.9578857421875\n",
      "Update 15: mean reward 0.9638671875\n",
      "Update 15: mean reward 0.9638671875\n",
      "Update 16: mean reward 0.96392822265625\n",
      "Update 16: mean reward 0.96392822265625\n",
      "Update 17: mean reward 0.969482421875\n",
      "Update 17: mean reward 0.969482421875\n",
      "Update 18: mean reward 0.9698486328125\n",
      "Update 18: mean reward 0.9698486328125\n",
      "Update 19: mean reward 0.9715576171875\n",
      "Update 19: mean reward 0.9715576171875\n",
      "Update 20: mean reward 0.97412109375\n",
      "Update 20: mean reward 0.97412109375\n",
      "Update 21: mean reward 0.9769287109375\n",
      "Update 21: mean reward 0.9769287109375\n",
      "Update 22: mean reward 0.9752197265625\n",
      "Update 22: mean reward 0.9752197265625\n",
      "Update 23: mean reward 0.9764404296875\n",
      "Update 23: mean reward 0.9764404296875\n",
      "Update 24: mean reward 0.976806640625\n",
      "Update 24: mean reward 0.976806640625\n",
      "Update 25: mean reward 0.9805908203125\n",
      "Update 25: mean reward 0.9805908203125\n",
      "Update 26: mean reward 0.9783935546875\n",
      "Update 26: mean reward 0.9783935546875\n",
      "Update 27: mean reward 0.9803466796875\n",
      "Update 27: mean reward 0.9803466796875\n",
      "Update 28: mean reward 0.9825439453125\n",
      "Update 28: mean reward 0.9825439453125\n",
      "Update 29: mean reward 0.9813232421875\n",
      "Update 29: mean reward 0.9813232421875\n",
      "Update 30: mean reward 0.9852294921875\n",
      "Update 30: mean reward 0.9852294921875\n",
      "Update 31: mean reward 0.9813232421875\n",
      "Update 31: mean reward 0.9813232421875\n",
      "Update 32: mean reward 0.9862060546875\n",
      "Update 32: mean reward 0.9862060546875\n",
      "Update 33: mean reward 0.98779296875\n",
      "Update 33: mean reward 0.98779296875\n",
      "Update 34: mean reward 0.9852294921875\n",
      "Update 34: mean reward 0.9852294921875\n",
      "Update 35: mean reward 0.9881591796875\n",
      "Update 35: mean reward 0.9881591796875\n",
      "Update 36: mean reward 0.989990234375\n",
      "Update 36: mean reward 0.989990234375\n",
      "Update 37: mean reward 0.9898681640625\n",
      "Update 37: mean reward 0.9898681640625\n",
      "Update 38: mean reward 0.9892578125\n",
      "Update 38: mean reward 0.9892578125\n",
      "Update 39: mean reward 0.991455078125\n",
      "Update 39: mean reward 0.991455078125\n",
      "Update 40: mean reward 0.9912109375\n",
      "Update 40: mean reward 0.9912109375\n",
      "Update 41: mean reward 0.9913330078125\n",
      "Update 41: mean reward 0.9913330078125\n",
      "Update 42: mean reward 0.99151611328125\n",
      "Update 42: mean reward 0.99151611328125\n",
      "Update 43: mean reward 0.99267578125\n",
      "Update 43: mean reward 0.99267578125\n",
      "Update 44: mean reward 0.99169921875\n",
      "Update 44: mean reward 0.99169921875\n",
      "Update 45: mean reward 0.99365234375\n",
      "Update 45: mean reward 0.99365234375\n",
      "Update 46: mean reward 0.99432373046875\n",
      "Update 46: mean reward 0.99432373046875\n",
      "Update 47: mean reward 0.994873046875\n",
      "Update 47: mean reward 0.994873046875\n",
      "Update 48: mean reward 0.99530029296875\n",
      "Update 48: mean reward 0.99530029296875\n",
      "Update 49: mean reward 0.9962158203125\n",
      "Update 49: mean reward 0.9962158203125\n"
     ]
    }
   ],
   "source": [
    "num_updates = 50\n",
    "reward_history = []\n",
    "for update in range(num_updates):\n",
    "    obs = envs.reset()[0]\n",
    "    obs_buf, act_buf, logp_buf, rew_buf, val_buf, done_buf = [], [], [], [], [], []\n",
    "    for step in range(steps_per_epoch):\n",
    "        action, logp, value = get_action_and_value(model, obs, device)\n",
    "        next_obs, reward, done, trunc, info = envs.step(action)\n",
    "        obs_buf.append(obs)\n",
    "        act_buf.append(action)\n",
    "        logp_buf.append(logp)\n",
    "        rew_buf.append(reward)\n",
    "        val_buf.append(value)\n",
    "        done_buf.append(done)\n",
    "        obs = next_obs\n",
    "    obs_buf = np.array(obs_buf)\n",
    "    act_buf = np.array(act_buf)\n",
    "    logp_buf = np.array(logp_buf)\n",
    "    rew_buf = np.array(rew_buf)\n",
    "    val_buf = np.array(val_buf)\n",
    "    if val_buf.ndim == 3 and val_buf.shape[2] == 1:\n",
    "        val_buf = val_buf.squeeze(-1)\n",
    "    done_buf = np.array(done_buf)\n",
    "    adv_buf = np.zeros_like(rew_buf)\n",
    "    for env_idx in range(envs.num_envs):\n",
    "        lastgaelam_env = 0\n",
    "        for t in reversed(range(steps_per_epoch)):\n",
    "            if t == steps_per_epoch - 1:\n",
    "                nextnonterminal = 1.0 - done_buf[t, env_idx]\n",
    "                nextvalue = val_buf[t, env_idx]\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - done_buf[t+1, env_idx]\n",
    "                nextvalue = val_buf[t+1, env_idx]\n",
    "            delta = rew_buf[t, env_idx] + gamma * nextvalue * nextnonterminal - val_buf[t, env_idx]\n",
    "            lastgaelam_env = delta + gamma * lam * nextnonterminal * lastgaelam_env\n",
    "            adv_buf[t, env_idx] = lastgaelam_env\n",
    "    returns = adv_buf + val_buf\n",
    "    obs_flat = torch.from_numpy(obs_buf.reshape(-1, obs_dim)).float().to(device)\n",
    "    act_flat = torch.from_numpy(act_buf.reshape(-1, act_dim)).float().to(device)\n",
    "    logp_flat = torch.from_numpy(logp_buf.flatten()).float().to(device)\n",
    "    adv_flat = torch.from_numpy(adv_buf.flatten()).float().to(device)\n",
    "    ret_flat = torch.from_numpy(returns.flatten()).float().to(device)\n",
    "    for _ in range(epochs):\n",
    "        mu, value = model(obs_flat)\n",
    "        std = model.log_std.exp()\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        new_logp = dist.log_prob(act_flat).sum(-1)\n",
    "        ratio = (new_logp - logp_flat).exp()\n",
    "        surr1 = ratio * adv_flat\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv_flat\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = ((ret_flat - value.squeeze()) ** 2).mean()\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    mean_reward = np.mean(rew_buf)\n",
    "    print(f\"Update {update}: mean reward {mean_reward}\")\n",
    "    reward_history.append(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d7e0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zle/miniconda3/envs/env_MFRL/lib/python3.13/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "render_env = gym.make(\"InvertedPendulum-v5\", render_mode=\"human\", disable_env_checker=True)\n",
    "obs = render_env.reset()[0]\n",
    "done = False\n",
    "while not done:\n",
    "    action, _, _ = get_action_and_value(model, obs, device)\n",
    "    obs, reward, done, trunc, info = render_env.step(action)\n",
    "    if done or trunc:\n",
    "        break\n",
    "render_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_MFRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
